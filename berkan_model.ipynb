{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nimport shutil\nimport os\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-15T15:33:52.810288Z","iopub.execute_input":"2022-03-15T15:33:52.810690Z","iopub.status.idle":"2022-03-15T15:33:57.914569Z","shell.execute_reply.started":"2022-03-15T15:33:52.810585Z","shell.execute_reply":"2022-03-15T15:33:57.913140Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"trainCSV = pd.read_csv(\"../input/happy-whale-and-dolphin/train.csv\", index_col=0)\n# species = trainCSV.species.tolist()\n\nspecies_classes = trainCSV[['species']].reset_index()\nspecies_unique = species_classes.species.unique()\nspecies_classes.loc[0].image","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:33:57.916590Z","iopub.execute_input":"2022-03-15T15:33:57.917176Z","iopub.status.idle":"2022-03-15T15:33:58.088878Z","shell.execute_reply.started":"2022-03-15T15:33:57.917128Z","shell.execute_reply":"2022-03-15T15:33:58.087713Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"if not os.path.isdir('../working/temp/train_images/'):\n    os.makedirs('../working/temp/train_images/')\n    \nfiles = sorted(os.listdir('../input/dolphin-resized/train_images_res_64x64/train_images_res_64x64'))\n\nfor x in range(len(files)):\n    if files[x] == species_classes.loc[x].image:\n        if not os.path.isdir('../working/temp/train_images/' + species_classes.loc[x].species):\n            os.makedirs('../working/temp/train_images/' + species_classes.loc[x].species)\n        \n        shutil.copy2('../input/dolphin-resized/train_images_res_64x64/train_images_res_64x64/' + files[x], '../working/temp/train_images/' + species_classes.loc[x].species + '/' + files[x])","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:33:58.095138Z","iopub.execute_input":"2022-03-15T15:33:58.096043Z","iopub.status.idle":"2022-03-15T15:39:11.703958Z","shell.execute_reply.started":"2022-03-15T15:33:58.095951Z","shell.execute_reply":"2022-03-15T15:39:11.703229Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\n# these are a new feature in TF 2.2\nfrom tensorflow.keras.layers.experimental import preprocessing\n\n# Load training and validation sets\ntrain_ = image_dataset_from_directory(\n    '../working/temp/train_images/',\n    labels='inferred',\n    image_size=[64, 64],\n    batch_size=128,\n    color_mode='grayscale',\n    crop_to_aspect_ratio=True,\n    seed=1,\n    validation_split=0.8,\n    subset='training',\n)\n\nval_ = image_dataset_from_directory(\n    '../working/temp/train_images/',\n    labels='inferred',\n    image_size=[64, 64],\n    batch_size=128,\n    color_mode='grayscale',\n    crop_to_aspect_ratio=True,\n    seed=1,\n    validation_split=0.2,\n    subset='validation',\n)\n\n# Data Pipeline\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain = (\n    train_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\nval = (\n    val_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:39:11.707666Z","iopub.execute_input":"2022-03-15T15:39:11.708351Z","iopub.status.idle":"2022-03-15T15:39:19.658915Z","shell.execute_reply.started":"2022-03-15T15:39:11.708311Z","shell.execute_reply":"2022-03-15T15:39:19.658267Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model = keras.Sequential([\n    preprocessing.RandomContrast(0.5),\n    preprocessing.RandomRotation(factor=0.20),\n    \n    # First Convolutional Block\n    layers.Conv2D(filters=64, kernel_size=5, activation=\"relu\", padding='same', input_shape=[64, 64, 1]),\n    layers.Dropout(0.3),\n    layers.MaxPool2D(),\n    \n    # Second Convolutional Block\n    layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.Dropout(0.3),\n    layers.MaxPool2D(),\n    \n    # Third Convolutional Block\n    layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.Dropout(0.3),\n    layers.MaxPool2D(),\n    \n    # Fourth Convolutional Block\n    layers.Conv2D(filters=512, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.Dropout(0.3),\n    layers.MaxPool2D(),\n    \n    layers.Flatten(),\n    layers.Dense(units=256, activation=\"relu\"),\n    layers.Dense(units=30, activation=\"softmax\"),\n])","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:39:19.660108Z","iopub.execute_input":"2022-03-15T15:39:19.660353Z","iopub.status.idle":"2022-03-15T15:39:19.708200Z","shell.execute_reply.started":"2022-03-15T15:39:19.660320Z","shell.execute_reply":"2022-03-15T15:39:19.707569Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer=keras.optimizers.Adam(epsilon=0.01),\n    loss='sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:39:19.709262Z","iopub.execute_input":"2022-03-15T15:39:19.711004Z","iopub.status.idle":"2022-03-15T15:39:19.724812Z","shell.execute_reply.started":"2022-03-15T15:39:19.710973Z","shell.execute_reply":"2022-03-15T15:39:19.724142Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train,\n    validation_data=val,\n    epochs=200,\n    callbacks=[early_stopping],\n    verbose=1,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:39:19.727415Z","iopub.execute_input":"2022-03-15T15:39:19.727611Z","iopub.status.idle":"2022-03-15T15:48:04.465414Z","shell.execute_reply.started":"2022-03-15T15:39:19.727586Z","shell.execute_reply":"2022-03-15T15:48:04.464685Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"history_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['sparse_categorical_accuracy', 'val_sparse_categorical_accuracy']].plot()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:48:04.466893Z","iopub.execute_input":"2022-03-15T15:48:04.467154Z","iopub.status.idle":"2022-03-15T15:48:04.874900Z","shell.execute_reply.started":"2022-03-15T15:48:04.467119Z","shell.execute_reply":"2022-03-15T15:48:04.874231Z"},"trusted":true},"execution_count":8,"outputs":[]}]}