{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: pydot in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.51.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pydot) (2.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz pydot tqdm \n",
    "!pip install --user wandb -qqq\n",
    "#!pip install -Iv protobuf==3.12.0\n",
    "#!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.11.0\n",
      "appdirs==1.4.4\n",
      "argon2-cffi==20.1.0\n",
      "astor==0.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astroid==2.5.3\n",
      "astunparse==1.6.3\n",
      "async-generator==1.10\n",
      "attrs==19.3.0\n",
      "autopep8==1.5.6\n",
      "backcall==0.2.0\n",
      "beautifulsoup4==4.9.0\n",
      "bleach==3.2.1\n",
      "branca==0.4.2\n",
      "bs4==0.0.1\n",
      "cachetools==4.0.0\n",
      "certifi==2021.10.8\n",
      "cffi==1.14.3\n",
      "chardet==3.0.4\n",
      "chromedriver-binary==95.0.4638.17.0\n",
      "chromedriver-binary-auto==0.1.1\n",
      "click==7.1.2\n",
      "color==0.1\n",
      "colorama==0.4.4\n",
      "comet-git-pure==0.19.14\n",
      "comet-ml==3.2.10\n",
      "configobj==5.0.6\n",
      "cryptography==35.0.0\n",
      "cycler==0.10.0\n",
      "DateTime==4.3\n",
      "decorator==4.4.1\n",
      "defusedxml==0.6.0\n",
      "deprecation==2.1.0\n",
      "dill==0.3.3\n",
      "distlib==0.3.1\n",
      "Django==2.2.17\n",
      "django-cors-headers==3.10.1\n",
      "djangorestframework==3.12.2\n",
      "docker-pycreds==0.4.0\n",
      "dulwich==0.20.15\n",
      "eikon==1.1.8\n",
      "entrypoints==0.3\n",
      "enum34==1.1.10\n",
      "et-xmlfile==1.0.1\n",
      "everett==1.0.2\n",
      "filelock==3.0.12\n",
      "findiff==0.8.9\n",
      "Flask==1.1.2\n",
      "flatbuffers==1.12\n",
      "folium==0.12.1\n",
      "future==0.18.2\n",
      "gast==0.3.3\n",
      "gitdb==4.0.9\n",
      "GitPython==3.1.27\n",
      "google-auth==1.11.2\n",
      "google-auth-oauthlib==0.4.1\n",
      "google-pasta==0.2.0\n",
      "graphviz==0.19.1\n",
      "grpcio==1.32.0\n",
      "h11==0.9.0\n",
      "h5py==2.10.0\n",
      "html5lib==1.0.1\n",
      "httpcore==0.10.2\n",
      "httpx==0.14.3\n",
      "idna==2.9\n",
      "imageio==2.6.1\n",
      "importlib-metadata==3.4.0\n",
      "ipykernel==5.3.4\n",
      "ipython==7.19.0\n",
      "ipython-genutils==0.2.0\n",
      "ipython-sparql-pandas==1.4\n",
      "isodate==0.6.0\n",
      "isort==5.8.0\n",
      "itsdangerous==1.1.0\n",
      "jdcal==1.4.1\n",
      "jedi==0.17.2\n",
      "Jinja2==2.11.2\n",
      "joblib==0.14.1\n",
      "json5==0.9.5\n",
      "jsonschema==3.0.2\n",
      "jupyter-client==6.1.7\n",
      "jupyter-core==4.6.3\n",
      "jupyterlab==2.2.9\n",
      "jupyterlab-pygments==0.1.2\n",
      "jupyterlab-server==1.2.0\n",
      "Keras==2.1.5\n",
      "Keras-Applications==1.0.8\n",
      "Keras-Preprocessing==1.1.2\n",
      "keras-tuner==1.0.2\n",
      "kiwisolver==1.1.0\n",
      "lazy-object-proxy==1.6.0\n",
      "lxml==4.6.3\n",
      "Markdown==3.2.1\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.1.2\n",
      "mccabe==0.6.1\n",
      "mistune==0.8.4\n",
      "MouseInfo==0.1.3\n",
      "mpmath==1.2.1\n",
      "multiprocess==0.70.11.1\n",
      "multitasking==0.0.9\n",
      "nbclient==0.5.1\n",
      "nbconvert==6.0.7\n",
      "nbformat==5.0.8\n",
      "nest-asyncio==1.3.3\n",
      "netifaces==0.10.9\n",
      "networkx==2.4\n",
      "notebook==6.1.4\n",
      "notebook-as-pdf==0.3.1\n",
      "numpy==1.19.5\n",
      "nvidia-ml-py3==7.352.0\n",
      "oauthlib==3.1.0\n",
      "opencv-python==4.4.0.46\n",
      "openpyxl==3.0.3\n",
      "opt-einsum==3.3.0\n",
      "outcome==1.1.0\n",
      "packaging==20.4\n",
      "pandas==1.0.0\n",
      "pandoc==1.0.2\n",
      "pandocfilters==1.4.3\n",
      "parso==0.7.1\n",
      "pathos==0.2.7\n",
      "pathtools==0.1.2\n",
      "pickleshare==0.7.5\n",
      "Pillow==7.0.0\n",
      "pipenv==2020.11.15\n",
      "ply==3.11\n",
      "pox==0.2.9\n",
      "ppft==1.6.6.3\n",
      "pptree==3.1\n",
      "prometheus-client==0.8.0\n",
      "promise==2.3\n",
      "prompt-toolkit==3.0.8\n",
      "protobuf==3.19.4\n",
      "psutil==5.9.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "PyAutoGUI==0.9.52\n",
      "pycodestyle==2.7.0\n",
      "pycparser==2.20\n",
      "pydot==1.4.2\n",
      "pyee==7.0.4\n",
      "PyGetWindow==0.0.9\n",
      "Pygments==2.7.2\n",
      "pylint==2.7.4\n",
      "PyMsgBox==1.0.7\n",
      "pyOpenSSL==21.0.0\n",
      "pyparsing==2.4.6\n",
      "PyPDF2==1.26.0\n",
      "pyperclip==1.8.1\n",
      "pyppeteer==0.2.2\n",
      "PyRect==0.1.4\n",
      "pyrsistent==0.15.7\n",
      "PyScreeze==0.1.26\n",
      "python-dateutil==2.8.1\n",
      "PyTweening==1.0.3\n",
      "pytz==2019.3\n",
      "PyWavelets==1.1.1\n",
      "pywin32==228\n",
      "pywinpty==0.5.7\n",
      "PyYAML==5.3\n",
      "pyzmq==19.0.2\n",
      "rdflib==6.0.0\n",
      "requests==2.23.0\n",
      "requests-oauthlib==1.3.0\n",
      "rfc3986==1.4.0\n",
      "rsa==4.0\n",
      "scikit-image==0.16.2\n",
      "scikit-learn==0.22.1\n",
      "scipy==1.6.0\n",
      "seaborn==0.11.1\n",
      "selenium==4.0.0\n",
      "Send2Trash==1.5.0\n",
      "sentry-sdk==1.5.5\n",
      "shortuuid==1.0.8\n",
      "six==1.15.0\n",
      "sklearn==0.0\n",
      "smmap==5.0.0\n",
      "sniffio==1.2.0\n",
      "sortedcontainers==2.4.0\n",
      "soupsieve==2.0\n",
      "SPARQLWrapper==1.8.5\n",
      "sqlparse==0.4.1\n",
      "sympy==1.8\n",
      "TA-Lib @ file:///D:/Downloads/TA_Lib-0.4.19-cp37-cp37m-win_amd64.whl\n",
      "tabulate==0.8.8\n",
      "tb-nightly==2.2.0a20200304\n",
      "tensorboard==2.4.1\n",
      "tensorboard-plugin-wit==1.6.0.post2\n",
      "tensorflow==2.4.1\n",
      "tensorflow-estimator==2.4.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.9.1\n",
      "terminaltables==3.1.0\n",
      "testpath==0.4.4\n",
      "tflearn==0.5.0\n",
      "toml==0.10.2\n",
      "tornado==6.1\n",
      "tqdm==4.51.0\n",
      "traitlets==5.0.5\n",
      "trendln==0.1.10\n",
      "trio==0.19.0\n",
      "trio-websocket==0.9.2\n",
      "typed-ast==1.4.3\n",
      "typing-extensions==3.7.4.3\n",
      "urllib3==1.25.11\n",
      "utils==1.0.1\n",
      "virtualenv==20.4.0\n",
      "virtualenv-clone==0.5.4\n",
      "wandb==0.12.10\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "websocket-client==0.57.0\n",
      "websockets==8.1\n",
      "Werkzeug==1.0.0\n",
      "wrapt==1.12.1\n",
      "wsproto==1.0.0\n",
      "wurlitzer==2.0.0\n",
      "xlrd==2.0.1\n",
      "yahoofinancials==1.6\n",
      "yaspin==2.1.0\n",
      "yfinance==0.1.59\n",
      "zipp==3.4.0\n",
      "zope.interface==5.2.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import imageio\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import save_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from graphviz import Digraph\n",
    "import pydot\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: dolphin_project (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\IPython\\html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/dolphin_project/dolphin_project%20/runs/3iexoqme\" target=\"_blank\">absurd-sea-1</a></strong> to <a href=\"https://wandb.ai/dolphin_project/dolphin_project%20\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/dolphin_project/dolphin_project%20/runs/3iexoqme?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1e003696248>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants\n",
    "BR = 0.2\n",
    "BATCH_SIZE = 16\n",
    "ROT, SCALE = 10, 0.1\n",
    "NUM_CLASSES = 26\n",
    "DESIRED_SIZE = (218, 145)\n",
    "TRAIN_DIR = \"I:/University/Courses/Machine Learning/dolphin_dataset/\"\n",
    "LR = 0.0001\n",
    "\n",
    "wandb.init(project=\"dolphin_project \",\n",
    "           config={\n",
    "               \"batch_size\": BATCH_SIZE,\n",
    "               \"dataset\": \"dolphin\",\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1a71fbb72250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>60008f293a2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4b00fe572063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>8e5253662392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51028</th>\n",
       "      <td>fff639a7a78b3f.jpg</td>\n",
       "      <td>beluga</td>\n",
       "      <td>5ac053677ed1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51029</th>\n",
       "      <td>fff8b32daff17e.jpg</td>\n",
       "      <td>cuviers_beaked_whale</td>\n",
       "      <td>1184686361b3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51030</th>\n",
       "      <td>fff94675cc1aef.jpg</td>\n",
       "      <td>blue_whale</td>\n",
       "      <td>5401612696b9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031</th>\n",
       "      <td>fffbc5dd642d8c.jpg</td>\n",
       "      <td>beluga</td>\n",
       "      <td>4000b3d7c24e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51032</th>\n",
       "      <td>fffdcd42312777.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4ddb2eeb5efb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51033 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image               species individual_id\n",
       "0      00021adfb725ed.jpg    melon_headed_whale  cadddb1636b9\n",
       "1      000562241d384d.jpg        humpback_whale  1a71fbb72250\n",
       "2      0007c33415ce37.jpg    false_killer_whale  60008f293a2b\n",
       "3      0007d9bca26a99.jpg    bottlenose_dolphin  4b00fe572063\n",
       "4      00087baf5cef7a.jpg        humpback_whale  8e5253662392\n",
       "...                   ...                   ...           ...\n",
       "51028  fff639a7a78b3f.jpg                beluga  5ac053677ed1\n",
       "51029  fff8b32daff17e.jpg  cuviers_beaked_whale  1184686361b3\n",
       "51030  fff94675cc1aef.jpg            blue_whale  5401612696b9\n",
       "51031  fffbc5dd642d8c.jpg                beluga  4000b3d7c24e\n",
       "51032  fffdcd42312777.jpg    bottlenose_dolphin  4ddb2eeb5efb\n",
       "\n",
       "[51033 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adjust names to fit\n",
    "train_csv = TRAIN_DIR + \"train.csv\"\n",
    "train_df = pd.read_csv(train_csv)\n",
    "train_df.species.replace({\"globis\": \"short_finned_pilot_whale\",\n",
    "                          \"pilot_whale\": \"short_finned_pilot_whale\",\n",
    "                          \"kiler_whale\": \"killer_whale\",\n",
    "                          \"bottlenose_dolpin\": \"bottlenose_dolphin\"}, inplace=True)\n",
    "\n",
    "species_labels = list(train_df.species.unique())\n",
    "images = train_df['image']\n",
    "sid = train_df['individual_id']\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.species.unique()\n",
    "len(train_df.species.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1a71fbb72250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>60008f293a2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>4b00fe572063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>8e5253662392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51028</th>\n",
       "      <td>fff639a7a78b3f.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>5ac053677ed1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51029</th>\n",
       "      <td>fff8b32daff17e.jpg</td>\n",
       "      <td>17</td>\n",
       "      <td>1184686361b3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51030</th>\n",
       "      <td>fff94675cc1aef.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>5401612696b9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031</th>\n",
       "      <td>fffbc5dd642d8c.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>4000b3d7c24e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51032</th>\n",
       "      <td>fffdcd42312777.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>4ddb2eeb5efb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51033 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image  species individual_id\n",
       "0      00021adfb725ed.jpg        0  cadddb1636b9\n",
       "1      000562241d384d.jpg        1  1a71fbb72250\n",
       "2      0007c33415ce37.jpg        2  60008f293a2b\n",
       "3      0007d9bca26a99.jpg        3  4b00fe572063\n",
       "4      00087baf5cef7a.jpg        1  8e5253662392\n",
       "...                   ...      ...           ...\n",
       "51028  fff639a7a78b3f.jpg        4  5ac053677ed1\n",
       "51029  fff8b32daff17e.jpg       17  1184686361b3\n",
       "51030  fff94675cc1aef.jpg        7  5401612696b9\n",
       "51031  fffbc5dd642d8c.jpg        4  4000b3d7c24e\n",
       "51032  fffdcd42312777.jpg        3  4ddb2eeb5efb\n",
       "\n",
       "[51033 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_id(sp):\n",
    "    return species_labels.index(sp)\n",
    "##encode species\n",
    "train_df[\"species\"] = train_df.apply(lambda row :get_id(row[\"species\"]),axis = 1)\n",
    "\n",
    "##one-hot encode species\n",
    "#train_df = pd.concat([train_df, pd.get_dummies(train_df[\"species\"],prefix='species_',drop_first=True)], axis = 1)\n",
    "#train_df.drop(['species'],axis=1, inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_rotate_image(im, sx, sy, deg_ccw):\n",
    "    im_orig = im\n",
    "    im = Image.new('RGBA', im_orig.size, (255, 255, 255, 255))\n",
    "    im.paste(im_orig)\n",
    "\n",
    "    w, h = im.size\n",
    "    angle = math.radians(-deg_ccw)\n",
    "\n",
    "    cos_theta = math.cos(angle)\n",
    "    sin_theta = math.sin(angle)\n",
    "\n",
    "    scaled_w, scaled_h = w * sx, h * sy\n",
    "\n",
    "    new_w = int(math.ceil(math.fabs(cos_theta * scaled_w) + math.fabs(sin_theta * scaled_h)))\n",
    "    new_h = int(math.ceil(math.fabs(sin_theta * scaled_w) + math.fabs(cos_theta * scaled_h)))\n",
    "\n",
    "    cx = w / 2.\n",
    "    cy = h / 2.\n",
    "    tx = new_w / 2.\n",
    "    ty = new_h / 2.\n",
    "\n",
    "    a = cos_theta / sx\n",
    "    b = sin_theta / sx\n",
    "    c = cx - tx * a - ty * b\n",
    "    d = -sin_theta / sy\n",
    "    e = cos_theta / sy\n",
    "    f = cy - tx * d - ty * e\n",
    "\n",
    "    return im.transform(\n",
    "        (new_w, new_h),\n",
    "        Image.AFFINE,\n",
    "        (a, b, c, d, e, f),\n",
    "        resample=Image.BILINEAR\n",
    "    )            \n",
    "\n",
    "def resize_with_crop_or_pad(im, process=False, flip=False, rotate_scale=None, br=None, non_square=None, crop=None):\n",
    "\n",
    "    old_size = im.size  # old_size[0] is in (width, height) format\n",
    "    max_dim = np.argmax(old_size)\n",
    "    ratio = float(DESIRED_SIZE[max_dim]) / old_size[max_dim]\n",
    "\n",
    "    #ratio = float(max(desired_size)) / max(old_size)\n",
    "    new_size = tuple([int(x * ratio) for x in old_size])\n",
    "    # use thumbnail() or resize() method to resize the input image\n",
    "\n",
    "    # thumbnail is a in-place operation\n",
    "\n",
    "    #im.thumbnail(new_size, Image.ANTIALIAS)\n",
    "    #im = im.resize(new_size, Image.ANTIALIAS)\n",
    "    #im = im.convert('RGB')\n",
    " \n",
    "    if crop is not None:\n",
    "        crop_value = 0.0\n",
    "        prob = np.random.uniform(0, 1)\n",
    "        if prob > 0.75:\n",
    "            crop_value = crop\n",
    "            im = ImageOps.crop(im, int(crop_value*im.size[1]))\n",
    "        elif prob > 0.5 and prob <= 0.75:\n",
    "            crop_value = crop*0.625\n",
    "        else:\n",
    "            crop_value = np.abs(np.random.uniform(0, 0.075))\n",
    "\n",
    "        im = ImageOps.crop(im, int(crop_value*im.size[1]))\n",
    "\n",
    "    if rotate_scale is not None:\n",
    "        sx, sy = np.random.normal(scale=rotate_scale[1])+1, np.random.normal(scale=rotate_scale[1])+1\n",
    "        r = np.random.normal(scale=rotate_scale[0])\n",
    "        im = scale_and_rotate_image(im, sx, sy, r)\n",
    "       \n",
    "\n",
    "    if br is not None:\n",
    "        b = np.random.normal(scale=br)+0.9\n",
    "        c = np.random.normal(scale=br)+0.9\n",
    "      \n",
    "        enhancerc = ImageEnhance.Contrast(im)\n",
    "        im = enhancerc.enhance(c)\n",
    "        enhancerb = ImageEnhance.Brightness(im)\n",
    "        im = enhancerb.enhance(b)\n",
    "        #im = im.resize((mobilenet_input_shape[0], mobilenet_input_shape[1]), Image.ANTIALIAS)\n",
    "\n",
    "    im = im.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "    if flip:\n",
    "        ran = np.random.random_sample()\n",
    "\n",
    "        if ran >= 0.5:\n",
    "            im = im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    #im.show()\n",
    "    # create a new image and paste the resized on it\n",
    "    if non_square is not None:\n",
    "        new_im = Image.new(\"RGB\", (DESIRED_SIZE[0], DESIRED_SIZE[1]))\n",
    "        new_im.paste(im, ((DESIRED_SIZE[0] - new_size[0]) // 2,\n",
    "                        (DESIRED_SIZE[1] - new_size[1]) // 2))\n",
    "        if process:\n",
    "            new_im = np.array(new_im, dtype=np.float32) / 255.\n",
    "        else:\n",
    "            new_im = np.array(new_im, dtype=np.float32)\n",
    "        \n",
    "        return new_im[:,:,:3]\n",
    "    \n",
    "    if process:\n",
    "        im = np.array(new_im, dtype=np.float32) / 255.\n",
    "    else:\n",
    "        im = np.array(im, dtype=np.float32)\n",
    "    \n",
    "    return im[:,:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' resizing dataset --- Create a folder train_images_sized and run this code one time to create the rescaled dataset\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    if os.path.isfile(TRAIN_DIR + \"train_images_sized/\" + train_df.iat[i, 0]):\n",
    "        continue\n",
    "    img = resize_with_crop_or_pad(\n",
    "                Image.open(TRAIN_DIR + \"train_images/\" + train_df.iat[i, 0]),\n",
    "                #process=True,\n",
    "                #flip=True,\n",
    "                #br=BR,\n",
    "                #rotate_scale=(ROT, SCALE),\n",
    "                non_square=True\n",
    "            )\n",
    "    img.save(TRAIN_DIR + \"train_images_sized/\" + train_df.iat[i, 0])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45732 5302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i,img in enumerate(tqdm(images)):\\n    image = cv2.imread(\"train_images/\"+img,cv2.IMREAD_GRAYSCALE)#imports pictures in grayscale since colors have own dimension\\n    image = cv2.resize(image, dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\\n    #dataset.append((image,sid[i]))\\n    dataset.append(image)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Data\n",
    "train_df_im_labels = train_df[[\"image\", \"species\"]]\n",
    "train_df_im_labels = train_df_im_labels.sample(frac=1, random_state=113)\n",
    "X_train, X_valid = train_df_im_labels.loc[:len(train_df_im_labels)*9//10], train_df_im_labels.loc[len(train_df_im_labels)*9//10:]\n",
    "print(len(X_train), len(X_valid))\n",
    "'''\n",
    "for i,img in enumerate(tqdm(images)):\n",
    "    image = cv2.imread(\"train_images/\"+img,cv2.IMREAD_GRAYSCALE)#imports pictures in grayscale since colors have own dimension\n",
    "    image = cv2.resize(image, dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "    #dataset.append((image,sid[i]))\n",
    "    dataset.append(image)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Data\n",
    "test_dir = \"test_images\"\n",
    "test_dataset = []\n",
    "for img in tqdm(os.listdir(test_dir)): \n",
    "    image = imageio.imread(\"test_images/\"+img)\n",
    "    image = cv2.resize(image, dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "    test_dataset.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, batch_idx):\n",
    "    batch = []\n",
    "    y = []\n",
    "    for idx in batch_idx:\n",
    "        img = resize_with_crop_or_pad(\n",
    "                Image.open(TRAIN_DIR + \"train_images_sized/\" + train_df_im_labels.iat[idx, 0]),\n",
    "                process=True,\n",
    "                flip=True,\n",
    "                br=BR,\n",
    "                rotate_scale=(ROT, SCALE),\n",
    "                non_square=True\n",
    "            )\n",
    "        #print(img.shape)\n",
    "        batch.append(\n",
    "            img\n",
    "        )\n",
    "        y.append(train_df_im_labels.iat[idx, 1])\n",
    "    \n",
    "    batch = np.array(batch)\n",
    "    #print(batch.shape)\n",
    "    y = to_categorical(y, NUM_CLASSES)\n",
    "    loss, acc = model.train_on_batch(batch, y)\n",
    "    wandb.log({\"loss\": loss, \"accuracy\": acc})\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, X_valid):\n",
    "    loss, acc = [], []\n",
    "    bs = 32\n",
    "    for b in range(len(math.ceil(X_valid/bs))):\n",
    "        for idx in range(bs):\n",
    "            img = resize_with_crop_or_pad(\n",
    "                    Image.open(TRAIN_DIR + \"train_images_sized/\" + X_valid.iat[b*bs+idx, 0]),\n",
    "                    process=True,\n",
    "                    flip=True,\n",
    "                    br=BR,\n",
    "                    rotate_scale=(ROT, SCALE),\n",
    "                    non_square=True\n",
    "                )\n",
    "            #print(img.shape)\n",
    "            batch.append(\n",
    "                img\n",
    "            )\n",
    "            y.append(train_df_im_labels.iat[b*bs+idx, 1])\n",
    "    \n",
    "        batch = np.array(batch)\n",
    "    #print(batch.shape)\n",
    "        y = to_categorical(y, NUM_CLASSES)\n",
    "        l, a = model.train_on_batch(batch, y)\n",
    "        loss.append(l)\n",
    "        acc.append(a)\n",
    "    return np.mean(loss), np.mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating model\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[64, 64]),\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(26, activation=\"softmax\"),\n",
    "])\n",
    "model.summary()\n",
    "plot_model(model,show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 145, 218, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 145, 218, 3)  0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " rescaling_1 (Rescaling)        (None, 145, 218, 3)  0           ['sequential_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 73, 109, 32)  896         ['rescaling_1[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 73, 109, 32)  128        ['conv2d_6[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 73, 109, 32)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 73, 109, 64)  18496       ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 73, 109, 64)  256        ['conv2d_7[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 73, 109, 64)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 73, 109, 64)  0           ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " separable_conv2d_9 (SeparableC  (None, 73, 109, 128  8896       ['activation_13[0][0]']          \n",
      " onv2D)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 73, 109, 128  512        ['separable_conv2d_9[0][0]']     \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 73, 109, 128  0           ['batch_normalization_13[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_10 (Separable  (None, 73, 109, 128  17664      ['activation_14[0][0]']          \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 73, 109, 128  512        ['separable_conv2d_10[0][0]']    \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 37, 55, 128)  0          ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 37, 55, 128)  8320        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 37, 55, 128)  0           ['max_pooling2d_4[0][0]',        \n",
      "                                                                  'conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 37, 55, 128)  0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_11 (Separable  (None, 37, 55, 256)  34176      ['activation_15[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 37, 55, 256)  1024       ['separable_conv2d_11[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 37, 55, 256)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_12 (Separable  (None, 37, 55, 256)  68096      ['activation_16[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 37, 55, 256)  1024       ['separable_conv2d_12[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 19, 28, 256)  0          ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 19, 28, 256)  33024       ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 19, 28, 256)  0           ['max_pooling2d_5[0][0]',        \n",
      "                                                                  'conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 19, 28, 256)  0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_13 (Separable  (None, 19, 28, 512)  133888     ['activation_17[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 19, 28, 512)  2048       ['separable_conv2d_13[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 19, 28, 512)  0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_14 (Separable  (None, 19, 28, 512)  267264     ['activation_18[0][0]']          \n",
      " Conv2D)                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 19, 28, 512)  2048       ['separable_conv2d_14[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 10, 14, 512)  0          ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 10, 14, 512)  131584      ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 10, 14, 512)  0           ['max_pooling2d_6[0][0]',        \n",
      "                                                                  'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 10, 14, 512)  0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " separable_conv2d_15 (Separable  (None, 10, 14, 728)  378072     ['activation_19[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 10, 14, 728)  2912       ['separable_conv2d_15[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 10, 14, 728)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_16 (Separable  (None, 10, 14, 728)  537264     ['activation_20[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 10, 14, 728)  2912       ['separable_conv2d_16[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 5, 7, 728)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 5, 7, 728)    373464      ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 5, 7, 728)    0           ['max_pooling2d_7[0][0]',        \n",
      "                                                                  'conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " separable_conv2d_17 (Separable  (None, 5, 7, 1024)  753048      ['add_7[0][0]']                  \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 5, 7, 1024)  4096        ['separable_conv2d_17[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 5, 7, 1024)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['activation_21[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 1024)         0           ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 26)           26650       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,808,274\n",
      "Trainable params: 2,799,538\n",
      "Non-trainable params: 8,736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_shape, num_classes):\n",
    "    # Note: input is flipped to (height, width) instead of (width, height)\n",
    "    inputs = keras.Input(shape=(input_shape[1], input_shape[0], input_shape[2]))\n",
    "    \n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            #version tf 2.4.1: \n",
    "            layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "            layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "        ]\n",
    "    )\n",
    "    # Image augmentation block\n",
    "    x = data_augmentation(inputs)\n",
    "    activation_str = \"elu\"\n",
    "    # Entry block\n",
    "    #version tf 2.4.1\n",
    "    x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(x)\n",
    "    #-----------------\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation_str)(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation_str)(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [128, 256, 512, 728]:\n",
    "        x = layers.Activation(activation_str)(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(activation_str)(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation_str)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "#creating model\n",
    "model = create_model([DESIRED_SIZE[0], DESIRED_SIZE[1], 3], NUM_CLASSES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling model\n",
    "model.compile(loss=keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer=keras.optimizers.Adam(LR),                    \n",
    "              metrics=[\"accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d2ba684acd0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss, acc: 3.397674322128296, 0.0\n",
      "train loss, acc: 2.4766085147857666, 0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#training model\n",
    "'''\n",
    "epochs = 20\n",
    "history = model.fit(X_train, y_train, epochs=epochs,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "#saving trained model\n",
    "model.save(\"trained_model_cnn.h5\")\n",
    "\n",
    "#with open('base_model.pkl','wb') as f:\n",
    "#    pickle.dump(model,f)\n",
    "'''\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    dataset_indexes_shuffled = np.random.permutation(np.arange(len(X_train)))\n",
    "    dataset_in_batches = [dataset_indexes_shuffled[i:i+BATCH_SIZE] \n",
    "                          for i in range(0, len(dataset_indexes_shuffled), BATCH_SIZE)]\n",
    "    len_dat_d4 = len(dataset_in_batches)//4\n",
    "    for i, batch in enumerate(dataset_in_batches):\n",
    "        loss, acc = train_on_batch(model, batch)\n",
    "        \n",
    "        if i in [len_dat_d4, len_dat_d4*2, len_dat_d4*3, len_dat_d4*4-1]:\n",
    "            loss_v, acc_v = validate(model, X_valid)\n",
    "            wandb.log({\"validation_loss\": loss_v, \"validation_accuracy\": acc_v})\n",
    "            print(f\"validation loss, acc: {loss_v}, {acc_v}\")\n",
    "        if i % 10 == 0:\n",
    "            print(f\"train loss, acc: {loss}, {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize model performance\n",
    "accuracy = history.history['sparse_categorical_accuracy']\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(range(epochs), accuracy, \"r\", label=\"Training Accuracy\")\n",
    "plt.plot(range(epochs), val_accuracy, \"orange\", label=\"Validation Accuracy\")\n",
    "plt.plot(range(epochs), loss, \"b\", label=\"Training Loss\")\n",
    "plt.plot(range(epochs), val_loss, \"g\", label=\"Validation Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "#plt.gca().set_ylim(0, 2)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model not in globals():\n",
    "    model = pickle.load(open('base_model.pkl', 'rb'))\n",
    "    \n",
    "y_proba = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
