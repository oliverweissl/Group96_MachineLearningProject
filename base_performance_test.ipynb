{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: pydot in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.51.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pydot) (2.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz pydot tqdm \n",
    "!pip install --user wandb -qqq\n",
    "#!pip install -Iv protobuf==3.12.0\n",
    "#!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import imageio\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from tensorflow.keras.regularizers import l2\n",
    "from graphviz import Digraph\n",
    "import pydot\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "import wandb\n",
    "\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: dolphin_project (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\IPython\\html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/dolphin_project/dolphin_project%20/runs/3vv8xurm\" target=\"_blank\">eternal-monkey-57</a></strong> to <a href=\"https://wandb.ai/dolphin_project/dolphin_project%20\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/dolphin_project/dolphin_project%20/runs/3vv8xurm?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x17db2e38488>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants\n",
    "BR = 0.2\n",
    "CROP = 0.05\n",
    "BATCH_SIZE = 32\n",
    "ROT, SCALE = 10, 0.1\n",
    "NUM_CLASSES = 26\n",
    "DESIRED_SIZE = (218, 145)\n",
    "TRAIN_DIR = \"I:/University/Courses/Machine Learning/dolphin_dataset/\"\n",
    "MODEL_SAVE_DIR = \"I:/University/Courses/Machine Learning/models/\"\n",
    "MODEL_FILE_NAME = \"dolphin_16_32_64s_swish\"#\"dolphin_cnn_128-256-512-512-1024s_1e-4_\"\n",
    "LR = 0.0001\n",
    "START_INDEX = 0\n",
    "START_EPOCH = 0\n",
    "EPOCHS = 10\n",
    "\n",
    "ACTIVATION_STR = \"swish\"\n",
    "FIRST_CONV_UNITS = 16\n",
    "SECOND_CONV_UNITS = 32\n",
    "CONV_UNITS_BODY = []#, 256, 512, 512]\n",
    "LAST_LAYER_UNITS = 64\n",
    "\n",
    "wandb.init(project=\"dolphin_project \",\n",
    "           config={\n",
    "               \"batch_size\": BATCH_SIZE,\n",
    "               \"lr\" : LR,\n",
    "               \"input_size\": DESIRED_SIZE,\n",
    "               \"dataset\": \"dolphin\",\n",
    "               \"activation\": ACTIVATION_STR,\n",
    "               \"conv_units\": [FIRST_CONV_UNITS, SECOND_CONV_UNITS] + CONV_UNITS_BODY + [LAST_LAYER_UNITS],\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1a71fbb72250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>60008f293a2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4b00fe572063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>8e5253662392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51028</th>\n",
       "      <td>fff639a7a78b3f.jpg</td>\n",
       "      <td>beluga</td>\n",
       "      <td>5ac053677ed1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51029</th>\n",
       "      <td>fff8b32daff17e.jpg</td>\n",
       "      <td>cuviers_beaked_whale</td>\n",
       "      <td>1184686361b3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51030</th>\n",
       "      <td>fff94675cc1aef.jpg</td>\n",
       "      <td>blue_whale</td>\n",
       "      <td>5401612696b9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031</th>\n",
       "      <td>fffbc5dd642d8c.jpg</td>\n",
       "      <td>beluga</td>\n",
       "      <td>4000b3d7c24e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51032</th>\n",
       "      <td>fffdcd42312777.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4ddb2eeb5efb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51033 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image               species individual_id\n",
       "0      00021adfb725ed.jpg    melon_headed_whale  cadddb1636b9\n",
       "1      000562241d384d.jpg        humpback_whale  1a71fbb72250\n",
       "2      0007c33415ce37.jpg    false_killer_whale  60008f293a2b\n",
       "3      0007d9bca26a99.jpg    bottlenose_dolphin  4b00fe572063\n",
       "4      00087baf5cef7a.jpg        humpback_whale  8e5253662392\n",
       "...                   ...                   ...           ...\n",
       "51028  fff639a7a78b3f.jpg                beluga  5ac053677ed1\n",
       "51029  fff8b32daff17e.jpg  cuviers_beaked_whale  1184686361b3\n",
       "51030  fff94675cc1aef.jpg            blue_whale  5401612696b9\n",
       "51031  fffbc5dd642d8c.jpg                beluga  4000b3d7c24e\n",
       "51032  fffdcd42312777.jpg    bottlenose_dolphin  4ddb2eeb5efb\n",
       "\n",
       "[51033 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adjust names to fit\n",
    "train_csv = TRAIN_DIR + \"train.csv\"\n",
    "train_df = pd.read_csv(train_csv)\n",
    "train_df.species.replace({\"globis\": \"short_finned_pilot_whale\",\n",
    "                          \"pilot_whale\": \"short_finned_pilot_whale\",\n",
    "                          \"kiler_whale\": \"killer_whale\",\n",
    "                          \"bottlenose_dolpin\": \"bottlenose_dolphin\"}, inplace=True)\n",
    "\n",
    "species_labels = list(train_df.species.unique())\n",
    "images = train_df['image']\n",
    "sid = train_df['individual_id']\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.species.unique()\n",
    "len(train_df.species.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1a71fbb72250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>60008f293a2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>4b00fe572063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>8e5253662392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51028</th>\n",
       "      <td>fff639a7a78b3f.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>5ac053677ed1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51029</th>\n",
       "      <td>fff8b32daff17e.jpg</td>\n",
       "      <td>17</td>\n",
       "      <td>1184686361b3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51030</th>\n",
       "      <td>fff94675cc1aef.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>5401612696b9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031</th>\n",
       "      <td>fffbc5dd642d8c.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>4000b3d7c24e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51032</th>\n",
       "      <td>fffdcd42312777.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>4ddb2eeb5efb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51033 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image  species individual_id\n",
       "0      00021adfb725ed.jpg        0  cadddb1636b9\n",
       "1      000562241d384d.jpg        1  1a71fbb72250\n",
       "2      0007c33415ce37.jpg        2  60008f293a2b\n",
       "3      0007d9bca26a99.jpg        3  4b00fe572063\n",
       "4      00087baf5cef7a.jpg        1  8e5253662392\n",
       "...                   ...      ...           ...\n",
       "51028  fff639a7a78b3f.jpg        4  5ac053677ed1\n",
       "51029  fff8b32daff17e.jpg       17  1184686361b3\n",
       "51030  fff94675cc1aef.jpg        7  5401612696b9\n",
       "51031  fffbc5dd642d8c.jpg        4  4000b3d7c24e\n",
       "51032  fffdcd42312777.jpg        3  4ddb2eeb5efb\n",
       "\n",
       "[51033 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_id(sp):\n",
    "    return species_labels.index(sp)\n",
    "##encode species\n",
    "train_df[\"species\"] = train_df.apply(lambda row :get_id(row[\"species\"]),axis = 1)\n",
    "\n",
    "##one-hot encode species\n",
    "#train_df = pd.concat([train_df, pd.get_dummies(train_df[\"species\"],prefix='species_',drop_first=True)], axis = 1)\n",
    "#train_df.drop(['species'],axis=1, inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_rotate_image(im, sx, sy, deg_ccw):\n",
    "    im_orig = im\n",
    "    im = Image.new('RGBA', im_orig.size, (255, 255, 255, 255))\n",
    "    im.paste(im_orig)\n",
    "\n",
    "    w, h = im.size\n",
    "    angle = math.radians(-deg_ccw)\n",
    "\n",
    "    cos_theta = math.cos(angle)\n",
    "    sin_theta = math.sin(angle)\n",
    "\n",
    "    scaled_w, scaled_h = w * sx, h * sy\n",
    "\n",
    "    new_w = int(math.ceil(math.fabs(cos_theta * scaled_w) + math.fabs(sin_theta * scaled_h)))\n",
    "    new_h = int(math.ceil(math.fabs(sin_theta * scaled_w) + math.fabs(cos_theta * scaled_h)))\n",
    "\n",
    "    cx = w / 2.\n",
    "    cy = h / 2.\n",
    "    tx = new_w / 2.\n",
    "    ty = new_h / 2.\n",
    "\n",
    "    a = cos_theta / sx\n",
    "    b = sin_theta / sx\n",
    "    c = cx - tx * a - ty * b\n",
    "    d = -sin_theta / sy\n",
    "    e = cos_theta / sy\n",
    "    f = cy - tx * d - ty * e\n",
    "\n",
    "    return im.transform(\n",
    "        (new_w, new_h),\n",
    "        Image.AFFINE,\n",
    "        (a, b, c, d, e, f),\n",
    "        resample=Image.BILINEAR\n",
    "    )            \n",
    "\n",
    "def resize_with_crop_or_pad(im, process=False, flip=False, rotate_scale=None, br=None, non_square=None, crop=None):\n",
    "\n",
    "    old_size = im.size  # old_size[0] is in (width, height) format\n",
    "    max_dim = np.argmax(old_size)\n",
    "    ratio = float(DESIRED_SIZE[max_dim]) / old_size[max_dim]\n",
    "\n",
    "    #ratio = float(max(desired_size)) / max(old_size)\n",
    "    new_size = tuple([int(x * ratio) for x in old_size])\n",
    "    # use thumbnail() or resize() method to resize the input image\n",
    "\n",
    "    # thumbnail is a in-place operation\n",
    "\n",
    "    #im.thumbnail(new_size, Image.ANTIALIAS)\n",
    "    #im = im.resize(new_size, Image.ANTIALIAS)\n",
    "    #im = im.convert('RGB')\n",
    " \n",
    "    if crop is not None:\n",
    "        crop_value = 0.0\n",
    "        prob = np.random.uniform(0, 1)\n",
    "        if prob > 0.75:\n",
    "            crop_value = crop\n",
    "            im = ImageOps.crop(im, int(crop_value*im.size[1]))\n",
    "        elif prob > 0.5 and prob <= 0.75:\n",
    "            crop_value = crop*0.625\n",
    "        else:\n",
    "            crop_value = np.abs(np.random.uniform(0, 0.075))\n",
    "\n",
    "        im = ImageOps.crop(im, int(crop_value*im.size[1]))\n",
    "    '''\n",
    "    if rotate_scale is not None:\n",
    "        sx, sy = np.random.normal(scale=rotate_scale[1])+1, np.random.normal(scale=rotate_scale[1])+1\n",
    "        r = np.random.normal(scale=rotate_scale[0])\n",
    "        im = scale_and_rotate_image(im, sx, sy, r)\n",
    "    ''' \n",
    "\n",
    "    if br is not None:\n",
    "        b = np.random.normal(scale=br)+0.9\n",
    "        c = np.random.normal(scale=br)+0.9\n",
    "      \n",
    "        enhancerc = ImageEnhance.Contrast(im)\n",
    "        im = enhancerc.enhance(c)\n",
    "        enhancerb = ImageEnhance.Brightness(im)\n",
    "        im = enhancerb.enhance(b)\n",
    "        #im = im.resize((mobilenet_input_shape[0], mobilenet_input_shape[1]), Image.ANTIALIAS)\n",
    "\n",
    "    im = im.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "    if flip:\n",
    "        ran = np.random.random_sample()\n",
    "\n",
    "        if ran >= 0.5:\n",
    "            im = im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    #im.show()\n",
    "    # create a new image and paste the resized on it\n",
    "    if non_square is not None:\n",
    "        new_im = Image.new(\"RGB\", (DESIRED_SIZE[0], DESIRED_SIZE[1]))\n",
    "        new_im.paste(im, ((DESIRED_SIZE[0] - new_size[0]) // 2,\n",
    "                        (DESIRED_SIZE[1] - new_size[1]) // 2))\n",
    "        if process:\n",
    "            new_im = np.array(new_im, dtype=np.float32) / 255.\n",
    "        else:\n",
    "            new_im = np.array(new_im, dtype=np.float32)\n",
    "        \n",
    "        return new_im[:,:,:3]\n",
    "    \n",
    "    if process:\n",
    "        im = np.array(new_im, dtype=np.float32) / 255.\n",
    "    else:\n",
    "        im = np.array(im, dtype=np.float32)\n",
    "    \n",
    "    return im[:,:,:3]\n",
    "\n",
    "def augment(im, br=None, crop=None):\n",
    "    if crop is not None:\n",
    "        crop_value = 0.0\n",
    "        prob = np.random.uniform(0, 1)\n",
    "        if prob > 0.75:\n",
    "            crop_value = crop\n",
    "            im = ImageOps.crop(im, int(crop_value*im.size[1]))\n",
    "        elif prob > 0.5 and prob <= 0.75:\n",
    "            crop_value = crop*0.625\n",
    "        else:\n",
    "            crop_value = np.abs(np.random.uniform(0, 0.075))\n",
    "\n",
    "        im = ImageOps.crop(im, int(crop_value*im.size[1]))\n",
    "        im = im.resize(DESIRED_SIZE, Image.ANTIALIAS)\n",
    "    \n",
    "    if br is not None:\n",
    "        b = np.random.normal(scale=br)+0.9\n",
    "        c = np.random.normal(scale=br)+0.9\n",
    "      \n",
    "        enhancerc = ImageEnhance.Contrast(im)\n",
    "        im = enhancerc.enhance(c)\n",
    "        enhancerb = ImageEnhance.Brightness(im)\n",
    "        im = enhancerb.enhance(b)\n",
    "    \n",
    "    im = np.array(im, dtype=np.float32)\n",
    "    return im[:,:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# resizing dataset --- Create a folder train_images_sized and run this code one time to create the rescaled dataset\\n# WARNING THIS WILL START MODIFYING THE IMAGES IN \"train_images_sized/\"\\nfrom PIL import ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES = True\\n\\ndef resize_with_crop_or_pad_for_resizing(im, process=False, flip=False, rotate_scale=None, br=None, non_square=None, crop=None):\\n\\n    old_size = im.size  # old_size[0] is in (width, height) format\\n    max_dim = np.argmax(old_size)\\n    ratio = float(DESIRED_SIZE[max_dim]) / old_size[max_dim]\\n\\n    #ratio = float(max(desired_size)) / max(old_size)\\n    new_size = tuple([int(x * ratio) for x in old_size])\\n    # use thumbnail() or resize() method to resize the input image\\n\\n    # thumbnail is a in-place operation\\n\\n    #im.thumbnail(new_size, Image.ANTIALIAS)\\n    #im = im.resize(new_size, Image.ANTIALIAS)\\n    #im = im.convert(\\'RGB\\')\\n \\n    if crop is not None:\\n        crop_value = 0.0\\n        prob = np.random.uniform(0, 1)\\n        if prob > 0.75:\\n            crop_value = crop\\n            im = ImageOps.crop(im, int(crop_value*im.size[1]))\\n        elif prob > 0.5 and prob <= 0.75:\\n            crop_value = crop*0.625\\n        else:\\n            crop_value = np.abs(np.random.uniform(0, 0.075))\\n\\n        im = ImageOps.crop(im, int(crop_value*im.size[1]))\\n    \\n    \\n    if rotate_scale is not None:\\n        sx, sy = np.random.normal(scale=rotate_scale[1])+1, np.random.normal(scale=rotate_scale[1])+1\\n        r = np.random.normal(scale=rotate_scale[0])\\n        im = scale_and_rotate_image(im, sx, sy, r)\\n    \\n\\n    if br is not None:\\n        b = np.random.normal(scale=br)+0.9\\n        c = np.random.normal(scale=br)+0.9\\n      \\n        enhancerc = ImageEnhance.Contrast(im)\\n        im = enhancerc.enhance(c)\\n        enhancerb = ImageEnhance.Brightness(im)\\n        im = enhancerb.enhance(b)\\n        #im = im.resize((mobilenet_input_shape[0], mobilenet_input_shape[1]), Image.ANTIALIAS)\\n\\n    im = im.resize(new_size, Image.ANTIALIAS)\\n    \\n    \\n    if flip:\\n        ran = np.random.random_sample()\\n\\n        if ran >= 0.5:\\n            im = im.transpose(Image.FLIP_LEFT_RIGHT)\\n    \\n    #im.show()\\n    # create a new image and paste the resized on it\\n    if non_square is not None:\\n        new_im = Image.new(\"RGB\", (DESIRED_SIZE[0], DESIRED_SIZE[1]))\\n        new_im.paste(im, ((DESIRED_SIZE[0] - new_size[0]) // 2,\\n                        (DESIRED_SIZE[1] - new_size[1]) // 2))\\n        #if process:\\n        #    new_im = np.array(new_im, dtype=np.float32) / 255.\\n        #else:\\n        #    new_im = np.array(new_im, dtype=np.float32)\\n        \\n        return new_im#[:,:,:3]\\n    \\n    if process:\\n        im = np.array(new_im, dtype=np.float32) / 255.\\n    else:\\n        im = np.array(im, dtype=np.float32)\\n    \\n    return im[:,:,:3]\\n\\nfor i in range(len(train_df)):\\n    if os.path.isfile(TRAIN_DIR + \"train_images_sized/\" + train_df.iat[i, 0]):\\n        continue\\n    img = resize_with_crop_or_pad_for_resizing(\\n                Image.open(TRAIN_DIR + \"train_images/\" + train_df.iat[i, 0]),\\n                non_square=True\\n            )\\n    img.save(TRAIN_DIR + \"train_images_sized/\" + train_df.iat[i, 0])\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# resizing dataset --- Create a folder train_images_sized and run this code one time to create the rescaled dataset\n",
    "# WARNING THIS WILL START MODIFYING THE IMAGES IN \"train_images_sized/\"\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def resize_with_crop_or_pad_for_resizing(im, process=False, flip=False, rotate_scale=None, br=None, non_square=None, crop=None):\n",
    "\n",
    "    old_size = im.size  # old_size[0] is in (width, height) format\n",
    "    max_dim = np.argmax(old_size)\n",
    "    ratio = float(DESIRED_SIZE[max_dim]) / old_size[max_dim]\n",
    "\n",
    "    #ratio = float(max(desired_size)) / max(old_size)\n",
    "    new_size = tuple([int(x * ratio) for x in old_size])\n",
    "    # use thumbnail() or resize() method to resize the input image\n",
    "\n",
    "    # thumbnail is a in-place operation\n",
    "\n",
    "    #im.thumbnail(new_size, Image.ANTIALIAS)\n",
    "    #im = im.resize(new_size, Image.ANTIALIAS)\n",
    "    #im = im.convert('RGB')\n",
    " \n",
    "    if crop is not None:\n",
    "        crop_value = 0.0\n",
    "        prob = np.random.uniform(0, 1)\n",
    "        if prob > 0.75:\n",
    "            crop_value = crop\n",
    "            im = ImageOps.crop(im, int(crop_value*im.size[1]))\n",
    "        elif prob > 0.5 and prob <= 0.75:\n",
    "            crop_value = crop*0.625\n",
    "        else:\n",
    "            crop_value = np.abs(np.random.uniform(0, 0.075))\n",
    "\n",
    "        im = ImageOps.crop(im, int(crop_value*im.size[1]))\n",
    "    \n",
    "    \n",
    "    if rotate_scale is not None:\n",
    "        sx, sy = np.random.normal(scale=rotate_scale[1])+1, np.random.normal(scale=rotate_scale[1])+1\n",
    "        r = np.random.normal(scale=rotate_scale[0])\n",
    "        im = scale_and_rotate_image(im, sx, sy, r)\n",
    "    \n",
    "\n",
    "    if br is not None:\n",
    "        b = np.random.normal(scale=br)+0.9\n",
    "        c = np.random.normal(scale=br)+0.9\n",
    "      \n",
    "        enhancerc = ImageEnhance.Contrast(im)\n",
    "        im = enhancerc.enhance(c)\n",
    "        enhancerb = ImageEnhance.Brightness(im)\n",
    "        im = enhancerb.enhance(b)\n",
    "        #im = im.resize((mobilenet_input_shape[0], mobilenet_input_shape[1]), Image.ANTIALIAS)\n",
    "\n",
    "    im = im.resize(new_size, Image.ANTIALIAS)\n",
    "    \n",
    "    \n",
    "    if flip:\n",
    "        ran = np.random.random_sample()\n",
    "\n",
    "        if ran >= 0.5:\n",
    "            im = im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    \n",
    "    #im.show()\n",
    "    # create a new image and paste the resized on it\n",
    "    if non_square is not None:\n",
    "        new_im = Image.new(\"RGB\", (DESIRED_SIZE[0], DESIRED_SIZE[1]))\n",
    "        new_im.paste(im, ((DESIRED_SIZE[0] - new_size[0]) // 2,\n",
    "                        (DESIRED_SIZE[1] - new_size[1]) // 2))\n",
    "        #if process:\n",
    "        #    new_im = np.array(new_im, dtype=np.float32) / 255.\n",
    "        #else:\n",
    "        #    new_im = np.array(new_im, dtype=np.float32)\n",
    "        \n",
    "        return new_im#[:,:,:3]\n",
    "    \n",
    "    if process:\n",
    "        im = np.array(new_im, dtype=np.float32) / 255.\n",
    "    else:\n",
    "        im = np.array(im, dtype=np.float32)\n",
    "    \n",
    "    return im[:,:,:3]\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    if os.path.isfile(TRAIN_DIR + \"train_images_sized/\" + train_df.iat[i, 0]):\n",
    "        continue\n",
    "    img = resize_with_crop_or_pad_for_resizing(\n",
    "                Image.open(TRAIN_DIR + \"train_images/\" + train_df.iat[i, 0]),\n",
    "                non_square=True\n",
    "            )\n",
    "    img.save(TRAIN_DIR + \"train_images_sized/\" + train_df.iat[i, 0])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45732 5302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i,img in enumerate(tqdm(images)):\\n    image = cv2.imread(\"train_images/\"+img,cv2.IMREAD_GRAYSCALE)#imports pictures in grayscale since colors have own dimension\\n    image = cv2.resize(image, dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\\n    #dataset.append((image,sid[i]))\\n    dataset.append(image)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Data\n",
    "train_df_im_labels = train_df[[\"image\", \"species\"]]\n",
    "train_df_im_labels = train_df_im_labels.sample(frac=1, random_state=113)\n",
    "X_train, X_valid = train_df_im_labels.loc[:len(train_df_im_labels)*9//10], train_df_im_labels.loc[len(train_df_im_labels)*9//10:]\n",
    "print(len(X_train), len(X_valid))\n",
    "'''\n",
    "for i,img in enumerate(tqdm(images)):\n",
    "    image = cv2.imread(\"train_images/\"+img,cv2.IMREAD_GRAYSCALE)#imports pictures in grayscale since colors have own dimension\n",
    "    image = cv2.resize(image, dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "    #dataset.append((image,sid[i]))\n",
    "    dataset.append(image)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Testing Data\\ntest_dir = \"test_images\"\\ntest_dataset = []\\nfor img in tqdm(os.listdir(test_dir)): \\n    image = imageio.imread(\"test_images/\"+img)\\n    image = cv2.resize(image, dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\\n    test_dataset.append(image)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Testing Data\n",
    "test_dir = \"test_images\"\n",
    "test_dataset = []\n",
    "for img in tqdm(os.listdir(test_dir)): \n",
    "    image = imageio.imread(\"test_images/\"+img)\n",
    "    image = cv2.resize(image, dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "    test_dataset.append(image)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(batch_idx):\n",
    "    batch = []\n",
    "    y = []\n",
    "    for idx in batch_idx:\n",
    "        img = augment(\n",
    "                Image.open(TRAIN_DIR + \"train_images_sized/\" + train_df_im_labels.iat[idx, 0]),\n",
    "                br=BR,\n",
    "                crop=CROP,\n",
    "                #rotate_scale=(ROT, SCALE),\n",
    "                #non_square=True\n",
    "            )\n",
    "        #print(img.shape)\n",
    "        batch.append(\n",
    "            img\n",
    "        )\n",
    "        y.append(train_df_im_labels.iat[idx, 1])\n",
    "    \n",
    "    batch = np.array(batch)\n",
    "    #print(batch.shape)\n",
    "    y = to_categorical(y, NUM_CLASSES)\n",
    "    loss, acc = model.train_on_batch(batch, y)\n",
    "    wandb.log({\"loss\": loss, \"accuracy\": acc})\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, X_valid):\n",
    "    loss, acc = [], []\n",
    "    bs = 32\n",
    "    for b in range(math.floor(len(X_valid)/bs)-1):\n",
    "        x, y = [], []\n",
    "        for idx in range(bs):\n",
    "            img = resize_with_crop_or_pad(\n",
    "                    Image.open(TRAIN_DIR + \"train_images_sized/\" + X_valid.iat[b*bs+idx, 0]),\n",
    "                    #process=True,\n",
    "                    #flip=True,\n",
    "                    #br=BR,\n",
    "                    #rotate_scale=(ROT, SCALE),\n",
    "                    non_square=True\n",
    "                )\n",
    "            #print(img.shape)\n",
    "            x.append(img)\n",
    "            y.append(train_df_im_labels.iat[b*bs+idx, 1])\n",
    "    \n",
    "        x = np.array(x)\n",
    "        #print(batch.shape)\n",
    "        y = to_categorical(y, NUM_CLASSES)\n",
    "        l, a = model.test_on_batch(x, y)\n",
    "        loss.append(l)\n",
    "        acc.append(a)\n",
    "    \n",
    "    x, y = [], []\n",
    "    for i in range(len(X_valid)//bs*bs, len(X_valid)):\n",
    "        img = resize_with_crop_or_pad(\n",
    "                Image.open(TRAIN_DIR + \"train_images_sized/\" + X_valid.iat[i, 0]),\n",
    "                #process=True,\n",
    "                #flip=True,\n",
    "                #br=BR,\n",
    "                #rotate_scale=(ROT, SCALE),\n",
    "                non_square=True\n",
    "            )\n",
    "        #print(img.shape)\n",
    "        x.append(img)\n",
    "        y.append(train_df_im_labels.iat[b*bs+idx, 1])\n",
    "    \n",
    "    x = np.array(x)\n",
    "    #print(batch.shape)\n",
    "    y = to_categorical(y, NUM_CLASSES)\n",
    "    l, a = model.test_on_batch(x, y)\n",
    "    loss.append(l)\n",
    "    acc.append(a)\n",
    "    \n",
    "    return np.mean(loss), np.mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = keras.Sequential([\\n    layers.Flatten(input_shape=[64, 64]),\\n    layers.Dense(512, activation=\"relu\"),\\n    layers.Dense(256, activation=\"relu\"),\\n    layers.Dense(128, activation=\"relu\"),\\n    layers.Dense(64, activation=\"relu\"),\\n    layers.Dense(26, activation=\"softmax\"),\\n])\\nmodel.summary()\\nplot_model(model,show_shapes=True, show_layer_names=True)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generating model\n",
    "'''\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[64, 64]),\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(26, activation=\"softmax\"),\n",
    "])\n",
    "model.summary()\n",
    "plot_model(model,show_shapes=True, show_layer_names=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 145, 218, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 145, 218, 3)       0         \n",
      "                                                                 \n",
      " rescaling (Rescaling)       (None, 145, 218, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 73, 109, 16)       448       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 73, 109, 16)      64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 73, 109, 16)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 73, 109, 32)       4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 73, 109, 32)      128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 73, 109, 32)       0         \n",
      "                                                                 \n",
      " separable_conv2d (Separable  (None, 73, 109, 64)      2400      \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 73, 109, 64)      256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 73, 109, 64)       0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 64)               0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 26)                1690      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,626\n",
      "Trainable params: 9,402\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_shape, num_classes):\n",
    "    # Note: input is flipped to (height, width) instead of (width, height)\n",
    "    inputs = keras.Input(shape=(input_shape[1], input_shape[0], input_shape[2]))\n",
    "    \n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            #version tf 2.4.1: \n",
    "            layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "            layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "        ]\n",
    "    )\n",
    "    # Image augmentation block\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    # Entry block\n",
    "    #version tf 2.4.1\n",
    "    x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(x)\n",
    "    #-----------------\n",
    "    x = layers.Conv2D(FIRST_CONV_UNITS, 3, strides=2, padding=\"same\", kernel_regularizer = l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(ACTIVATION_STR)(x)\n",
    "\n",
    "    x = layers.Conv2D(SECOND_CONV_UNITS, 3, padding=\"same\", kernel_regularizer = l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(ACTIVATION_STR)(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in CONV_UNITS_BODY:\n",
    "        x = layers.Activation(ACTIVATION_STR)(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\", kernel_regularizer = l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(ACTIVATION_STR)(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\", kernel_regularizer = l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", kernel_regularizer = l2(1e-4))(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(LAST_LAYER_UNITS, 3, padding=\"same\", kernel_regularizer = l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(ACTIVATION_STR)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "'''\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(input_shape[1], input_shape[0], input_shape[2])),\n",
    "        layers.Dense(512, activation=\"elu\", kernel_regularizer=l2(1e-4)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(256, activation=\"elu\", kernel_regularizer=l2(1e-4)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(128, activation=\"elu\", kernel_regularizer=l2(1e-4)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(64, activation=\"elu\", kernel_regularizer=l2(1e-4)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = tf.keras.applications.DenseNet121(\n",
    "        include_top=True,\n",
    "        weights=None,\n",
    "        input_shape=(input_shape[1], input_shape[0], input_shape[2]),\n",
    "        pooling=None,\n",
    "        classes=num_classes,\n",
    "    )\n",
    "    return model\n",
    "'''\n",
    "#creating model\n",
    "model = create_model([DESIRED_SIZE[0], DESIRED_SIZE[1], 3], NUM_CLASSES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling model\n",
    "model.compile(loss=keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer=keras.optimizers.Adam(LR),                    \n",
    "              metrics=[\"accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss, acc: 3.4892385005950928, 0.0\n",
      "------- Iteration 0 -----------\n",
      "train loss, acc: 3.282214641571045, 0.0625\n",
      "train loss, acc: 3.4152374267578125, 0.03125\n",
      "train loss, acc: 3.249082326889038, 0.0\n",
      "train loss, acc: 3.348989486694336, 0.0625\n",
      "train loss, acc: 3.267744541168213, 0.0625\n",
      "train loss, acc: 3.49948787689209, 0.03125\n",
      "train loss, acc: 3.272313117980957, 0.09375\n",
      "train loss, acc: 3.322852611541748, 0.0625\n",
      "train loss, acc: 3.080648422241211, 0.0625\n",
      "train loss, acc: 3.1222431659698486, 0.15625\n",
      "------- Iteration 100 -----------\n",
      "train loss, acc: 2.990386724472046, 0.125\n",
      "train loss, acc: 2.800769567489624, 0.28125\n",
      "train loss, acc: 2.9225258827209473, 0.125\n",
      "train loss, acc: 3.140883207321167, 0.1875\n",
      "train loss, acc: 2.812772274017334, 0.1875\n",
      "train loss, acc: 3.03123140335083, 0.09375\n",
      "train loss, acc: 3.2850193977355957, 0.0625\n",
      "train loss, acc: 2.898447275161743, 0.1875\n",
      "train loss, acc: 2.8071017265319824, 0.25\n",
      "train loss, acc: 2.877318859100342, 0.125\n",
      "------- Iteration 200 -----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bf201fc01b76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mlen_dat_d4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_in_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_in_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSTART_INDEX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mSTART_INDEX\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen_dat_d4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_dat_d4\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_dat_d4\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_dat_d4\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-bf26b95fa5e9>\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(batch_idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#print(batch.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2089\u001b[0m       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n\u001b[0;32m   2090\u001b[0m                                                     \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2091\u001b[1;33m                                                     class_weight)\n\u001b[0m\u001b[0;32m   2092\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2093\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msingle_batch_iterator\u001b[1;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1639\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_distribute_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1641\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[1;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m             \"not be specified.\")\n\u001b[1;32m--> 755\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    785\u001b[0m                 \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m                 output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m         \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m         self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3313\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3314\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 3315\u001b[1;33m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[0;32m   3316\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3317\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#training model\n",
    "'''\n",
    "epochs = 20\n",
    "history = model.fit(X_train, y_train, epochs=epochs,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "#saving trained model\n",
    "model.save(\"trained_model_cnn.h5\")\n",
    "\n",
    "#with open('base_model.pkl','wb') as f:\n",
    "#    pickle.dump(model,f)\n",
    "'''\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#model = load_model(MODEL_SAVE_DIR + f\"trained_model_cnn_batch_{start_index}.h5\")\n",
    "\n",
    "for e in range(START_EPOCH, EPOCHS):\n",
    "    dataset_indexes_shuffled = np.random.permutation(np.arange(len(X_train)))\n",
    "    dataset_in_batches = [dataset_indexes_shuffled[i:i+BATCH_SIZE] \n",
    "                          for i in range(0, len(dataset_indexes_shuffled), BATCH_SIZE)]\n",
    "    len_dat_d4 = len(dataset_in_batches)//4\n",
    "    for i, batch in enumerate(dataset_in_batches[START_INDEX:]):\n",
    "        loss, acc = train_on_batch(batch)\n",
    "        \n",
    "        if i+START_INDEX in [len_dat_d4, len_dat_d4*2, len_dat_d4*3, len_dat_d4*4-1]:\n",
    "            loss_v, acc_v = validate(model, X_valid)\n",
    "            wandb.log({\"validation_loss\": loss_v, \"validation_accuracy\": acc_v})\n",
    "            print(f\"validation loss, acc: {loss_v}, {acc_v}\")\n",
    "        if i % 10 == 0:\n",
    "            print(f\"train loss, acc: {loss}, {acc}\")\n",
    "        if i % 100 == 0:\n",
    "            print(f\"------- Iteration {i+START_INDEX} -----------\")\n",
    "            model.save(MODEL_SAVE_DIR + MODEL_FILE_NAME + f\"{i+START_INDEX}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize model performance\n",
    "accuracy = history.history['sparse_categorical_accuracy']\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(range(epochs), accuracy, \"r\", label=\"Training Accuracy\")\n",
    "plt.plot(range(epochs), val_accuracy, \"orange\", label=\"Validation Accuracy\")\n",
    "plt.plot(range(epochs), loss, \"b\", label=\"Training Loss\")\n",
    "plt.plot(range(epochs), val_loss, \"g\", label=\"Validation Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "#plt.gca().set_ylim(0, 2)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model not in globals():\n",
    "    model = pickle.load(open('base_model.pkl', 'rb'))\n",
    "    \n",
    "y_proba = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
